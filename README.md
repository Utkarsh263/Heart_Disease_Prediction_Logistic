# â¤ï¸ Heart Disease Prediction using Logistic Regression (Recall Optimized)

## ğŸ“Œ Overview
Heart disease prediction is a high-risk medical classification problem where **missing a patient (False Negative)** can be life-threatening.  
This project focuses on building a **Logistic Regression model optimized for maximum Recall**, ensuring that most heart disease cases are correctly detected.

---

## ğŸ§  Problem Statement
Given clinical and demographic patient data, predict whether a patient has heart disease.

Target variable:
- 1 â†’ Heart disease present
- 0 â†’ No heart disease

The main objective is **high Recall**, not just high Accuracy.

---

## ğŸš¨ Why Recall is the Priority
In medical diagnosis:
- False Negative (FN) = patient has disease but model predicts healthy âŒ
- False Positives (FP) are acceptable compared to FN

Recall formula:

Recall = TP / (TP + FN)

Higher Recall â‡’ fewer missed patients â¤ï¸

---

## âŒ Why Basic Logistic Regression is Not Enough
A basic Logistic Regression model:
- Optimizes **log loss**, not medical risk
- Assumes balanced classes
- Can overfit training data
- Depends heavily on a single train-test split

This may give good accuracy but **poor Recall**, which is unsafe in healthcare.

---

## ğŸ“‰ Logistic Regression Basics

### Sigmoid Function
The model predicts probability using:

p = 1 / (1 + e^(-z))

where:

z = w0 + w1x1 + w2x2 + ... + wnxn

---

### Log Loss (Binary Cross-Entropy)
Basic Logistic Regression minimizes:

Log Loss = 
- [ y Â· log(p) + (1 âˆ’ y) Â· log(1 âˆ’ p) ]

This loss treats all errors equally âŒ  
Medical problems do not.

---

## ğŸ›¡ï¸ Why Regularization is Needed
Large weights cause overfitting.  
To prevent this, **L2 Regularization** is used.

L2 penalty:

L2 = w1Â² + w2Â² + ... + wnÂ²

---

### Final Loss Function
Regularized loss becomes:

Total Loss = Log Loss + (1 / C) Ã— L2

Where:
- C = inverse regularization strength
- Small C â†’ strong regularization
- Large C â†’ weak regularization

This improves generalization âœ…

---

## âš–ï¸ Handling Class Imbalance
Heart disease datasets usually have **fewer positive cases**.

To fix this, class weights are applied:

Class Weight = Total Samples / (2 Ã— Samples in Class)

Effect:
- Minority class (heart disease) gets higher importance
- False Negatives are penalized more
- Recall improves significantly ğŸ’ª

---

## ğŸ” Why Cross-Validation is Required
Instead of relying on a single split, k-Fold Cross-Validation is used.

If dataset size = N and k = 5:

Fold size = N / 5

Each fold:
- Trains on 4 folds
- Validates on 1 fold

Benefits:
- Stable performance estimation
- Reliable Recall score
- Better generalization ğŸ§ª

---

## ğŸ¯ Hyperparameter Tuning (Recall-Focused)
There is no universal best value for C.

Therefore:
- Multiple values of C are tested
- Performance is evaluated using **Recall**
- Best parameter is chosen using Cross-Validation

This ensures Recall is **consistently high**, not accidental.

---

## ğŸ“Š Evaluation Metrics

### Confusion Matrix

|              | Predicted 0 | Predicted 1 |
|-------------|------------|------------|
| Actual 0    | TN         | FP         |
| Actual 1    | FN âŒ      | TP âœ…      |

---

### Metric Formulas

Accuracy = (TP + TN) / (TP + TN + FP + FN)

Precision = TP / (TP + FP)

Recall = TP / (TP + FN) â­

Primary goal â†’ **minimize FN**

---

## âœ… Key Outcome
By combining:
- Regularization
- Class weight balancing
- Feature scaling
- Cross-validation
- Recall-based tuning

The model becomes **safe, reliable, and suitable for medical diagnosis** ğŸ¥

---

## ğŸ§¾ Conclusion
This project demonstrates why naive machine learning models are insufficient for healthcare applications. By prioritizing Recall and reducing False Negatives, the model aligns with real-world medical decision-making and patient safety.

---

## ğŸ‘¨â€ğŸ’» Author
**Utkarsh Kohli**
